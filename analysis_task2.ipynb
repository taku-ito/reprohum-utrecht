{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8966ed",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfd079",
   "metadata": {},
   "source": [
    "## check annotation\n",
    "After requesting re-annotation, there should be no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6123023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.feature_selection import r_regression\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from typing import List\n",
    "import json\n",
    "from util import read_excelfile, detect_annotation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c25ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = read_excelfile('data/participant1_task2.xlsx')\n",
    "df_p2 = read_excelfile('data/participant2_task2.xlsx')\n",
    "df_p3 = read_excelfile('data/participant3_task2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c8e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: there are no annotation errors\n"
     ]
    }
   ],
   "source": [
    "for df in (df_p1, df_p2, df_p3):\n",
    "    errors = detect_annotation_errors(df)\n",
    "    assert all([len(el) == 0 for el in errors.values()]), f'There are annotation errors: {errors}'\n",
    "print('INFO: there are no annotation errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8709c5",
   "metadata": {},
   "source": [
    "## Score and T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15eafb",
   "metadata": {},
   "source": [
    "### load data\n",
    "- order: human_eval_order.json\n",
    "- df_p1_r1: roudn1 data of participant1\n",
    "- df_p2_r1: roudn1 data of participant2\n",
    "- df_p3_r1: roudn1 data of participant3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e26f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = json.load(open('data/human_eval_order.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a12269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1_r1 = read_excelfile('data/participant1_task1.xlsx')\n",
    "df_p2_r1 = read_excelfile('data/participant2_task1.xlsx')\n",
    "df_p3_r1 = read_excelfile('data/participant3_task1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382dd54b",
   "metadata": {},
   "source": [
    "Any potential errors in round-1 annotations should have been found in notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713b083",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_criterion(criterion, data) -> List[float]:\n",
    "    assert criterion in ['Informativeness', 'Non-redundancy', 'Fluency']\n",
    "    scores = []\n",
    "    for line in list(data[criterion]):\n",
    "        if line != criterion:\n",
    "            if isinstance(line, str):\n",
    "                nums = list(map(int, line.replace(' ', '').replace('ï¼Œ', ',').split(',')))\n",
    "                assert all([el in [0, 1, 2] for el in nums])\n",
    "                ave = sum(nums) / len(nums)\n",
    "                scores.append(ave)\n",
    "            else:\n",
    "                assert line in [0, 1, 2]\n",
    "                scores.append(line)\n",
    "    return scores\n",
    "\n",
    "def split_list(my_list, n):\n",
    "    for idx in range(0, len(my_list), n):\n",
    "        yield my_list[idx:idx + n]\n",
    "        \n",
    "def c_order(order, data):\n",
    "    # first batch\n",
    "    # [1,0] = [PGN-both, PGN-multi]\n",
    "    # [0,1] = [PGN-multi, PGN-both]\n",
    "    # second batch\n",
    "    # [1,0] = [BERT-both, BERT-multi]\n",
    "    # [0,1] = [BERT-multi, BERT-both]\n",
    "    # ie\n",
    "    # [[1, 0], [0, 1]] = [PGN-both, PGN-multi, BERT-multi, BERT-both]\n",
    "    # [[0, 1], [1, 0]] = [PGN-multi, PGN-both, BERT-both, BERT-multi]\n",
    "    # [[1, 0], [1, 0]] = [PGN-both, PGN-multi, BERT-both, BERT-multi]\n",
    "    # [[0, 1], [0, 1]] = [PGN-multi, PGN-both, BERT-multi, BERT-both]\n",
    "        \n",
    "    assert len(order) == len(order[0]) == len(order[1]) == 2\n",
    "    assert len(data) == 4\n",
    "    \n",
    "    # order[0] is PGN\n",
    "    if order[0] == [0, 1]:\n",
    "        PGNmulti = data[0]\n",
    "        PGNboth = data[1]\n",
    "    elif order[0] == [1, 0]:\n",
    "        PGNmulti = data[1]\n",
    "        PGNboth = data[0]\n",
    "        \n",
    "    # order[1] is BERT\n",
    "    if order[1] == [0, 1]:\n",
    "        BERTmulti = data[2]\n",
    "        BERTboth = data[3]\n",
    "    elif order[1] == [1, 0]:\n",
    "        BERTmulti = data[3]\n",
    "        BERTboth = data[2]\n",
    "    return {'PGN-multi': PGNmulti, 'PGN-both': PGNboth, 'BERT-multi': BERTmulti, 'BERT-both': BERTboth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aad91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data: pd.DataFrame, order):\n",
    "    criteria = ['Informativeness', 'Non-redundancy', 'Fluency']\n",
    "    user_dfs = []\n",
    "    agent_dfs = []\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        # There are 2 architectures (PGN, BERT), 2 models (multi, both), 2 roles (agent, user)\n",
    "        # These are 8 values per summary\n",
    "        # Here we extract the list of values per summary\n",
    "        chunk8s = [c for c in split_list(extract_criterion(criterion, data), 8)]\n",
    "        assert len(chunk8s) == 100\n",
    "        assert all([len(el) == 8 for el in chunk8s])\n",
    "\n",
    "        user_data = []\n",
    "        agent_data = []\n",
    "        \n",
    "        assert len(chunk8s) == len(order['user']) == len(order['agent'])\n",
    "        \n",
    "        for chunk8, user_order, agent_order in zip(chunk8s, order['user'], order['agent']):\n",
    "            # user: 4 instance \n",
    "            # agent: 4 instance\n",
    "            # User comes before agent (see documentation on Google Drive)\n",
    "            user_scores = chunk8[0:4]\n",
    "            agent_scores = chunk8[4:8]\n",
    "            assert len(user_scores) == len(agent_scores) == 4\n",
    "            out_user = c_order(user_order, user_scores)\n",
    "            user_data.append(out_user)\n",
    "            \n",
    "            out_agent = c_order(agent_order, agent_scores)\n",
    "            agent_data.append(out_agent)\n",
    "            \n",
    "        user_df = pd.DataFrame(user_data)\n",
    "        user_df.columns = [f\"{criterion}_{m}_user\" for m in user_df.columns]\n",
    "        user_dfs.append(user_df)\n",
    "        \n",
    "        agent_df = pd.DataFrame(agent_data)\n",
    "        agent_df.columns = [f\"{criterion}_{m}_agent\" for m in agent_df.columns]\n",
    "        agent_dfs.append(agent_df)\n",
    "        \n",
    "    combined_user_df = pd.concat(user_dfs, axis=1)\n",
    "    combined_agent_df = pd.concat(agent_dfs, axis=1)\n",
    "    combined_df = pd.concat([combined_user_df, combined_agent_df], axis=1)\n",
    "    \n",
    "    # cal_overall\n",
    "    for model in  ['PGN-multi', 'PGN-both', 'BERT-multi', 'BERT-both']:\n",
    "        for u_or_a in ['user', 'agent']:\n",
    "            cols = [f\"{c}_{model}_{u_or_a}\" for c in criteria]\n",
    "            combined_df = pd.concat((combined_df, combined_df[cols].mean(axis=1).rename(f\"Overall_{model}_{u_or_a}\")), axis=1)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a299ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(df) -> dict:\n",
    "    criteria = ['Informativeness', 'Non-redundancy', 'Fluency', 'Overall']\n",
    "    models = ['PGN', 'BERT']\n",
    "    u_or_a = ['user', 'agent']\n",
    "    criterion_results = {}\n",
    "    for criterion in criteria:\n",
    "        model_results = {}\n",
    "        for model in models:\n",
    "            role_results = {}\n",
    "            for u_or_a in ['user', 'agent']:\n",
    "                test_res = stats.ttest_rel(df[f\"{criterion}_{model}-both_{u_or_a}\"], \n",
    "                                           df[f\"{criterion}_{model}-multi_{u_or_a}\"], \n",
    "                                           alternative='two-sided')\n",
    "                role_results[u_or_a] = test_res\n",
    "            model_results[model] = role_results\n",
    "        criterion_results[criterion] = model_results\n",
    "    return criterion_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bd5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(value: float, p_value: float) -> str:\n",
    "    return f\"{value:.2f}{'*' if abs(p_value) < 0.05 else ''}\"\n",
    "\n",
    "def report_results(data_df: pd.DataFrame) -> None:\n",
    "    results = (data_df.mean(axis=0) / 2).to_dict()\n",
    "    p_values = ttest(data_df)\n",
    "    print('CSDS\\t\\tInfo\\t\\tNon-Red\\t\\tFlu\\t\\tOverall')\n",
    "    for model in ('PGN-multi', 'PGN-both', 'BERT-multi', 'BERT-both'):\n",
    "        number_cells = ['/'.join([format_number(results[f'{metric}_{model}_{role}'], \n",
    "                                                p_values[metric][model.split('-')[0]][role].pvalue) \\\n",
    "                                  for role in ('user', 'agent')]) \\\n",
    "                        for metric in ('Informativeness', 'Non-redundancy', 'Fluency', 'Overall')]\n",
    "        print(model + '\\t' + '\\t'.join(number_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57fd12",
   "metadata": {},
   "source": [
    "## Case1\n",
    "Data for 1-10 were annotated by Participant1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83aff4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSDS\t\tInfo\t\tNon-Red\t\tFlu\t\tOverall\n",
      "PGN-multi\t0.63/0.59*\t0.58/0.55\t0.69/0.70\t0.63/0.61*\n",
      "PGN-both\t0.62/0.64*\t0.61/0.59\t0.68/0.74\t0.64/0.65*\n",
      "BERT-multi\t0.55/0.45\t0.69*/0.61\t0.82/0.80\t0.69*/0.62\n",
      "BERT-both\t0.56/0.47\t0.62*/0.58\t0.78/0.80\t0.65*/0.62\n"
     ]
    }
   ],
   "source": [
    "all_data_r1p1 = pd.concat([df_p1_r1, df_p1, df_p2, df_p3])\n",
    "df1 = read_data(all_data_r1p1, order)\n",
    "report_results(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d7235",
   "metadata": {},
   "source": [
    "## Case2\n",
    "Data for 1-10 were annotated by Participant2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63042e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSDS\t\tInfo\t\tNon-Red\t\tFlu\t\tOverall\n",
      "PGN-multi\t0.62/0.58\t0.57/0.56\t0.68/0.69\t0.62/0.61*\n",
      "PGN-both\t0.61/0.62\t0.60/0.58\t0.67/0.71\t0.63/0.64*\n",
      "BERT-multi\t0.55/0.45\t0.70*/0.60\t0.82/0.78\t0.69*/0.61\n",
      "BERT-both\t0.55/0.47\t0.62*/0.57\t0.78/0.78\t0.65*/0.61\n"
     ]
    }
   ],
   "source": [
    "all_data_r1p2 = pd.concat([df_p2_r1, df_p1, df_p2, df_p3])\n",
    "df2 = read_data(all_data_r1p2, order)\n",
    "report_results(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbe121",
   "metadata": {},
   "source": [
    "## Case3\n",
    "Data for 1-10 were annotated by Participant3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad360510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSDS\t\tInfo\t\tNon-Red\t\tFlu\t\tOverall\n",
      "PGN-multi\t0.64/0.60*\t0.59/0.58\t0.69/0.72\t0.64/0.63*\n",
      "PGN-both\t0.63/0.65*\t0.62/0.60\t0.68/0.75\t0.64/0.67*\n",
      "BERT-multi\t0.57/0.46\t0.72*/0.62\t0.83/0.81\t0.71*/0.63\n",
      "BERT-both\t0.57/0.49\t0.63*/0.59\t0.79/0.80\t0.67*/0.63\n"
     ]
    }
   ],
   "source": [
    "all_data_r1p3 = pd.concat([df_p3_r1, df_p1, df_p2, df_p3])\n",
    "df3 = read_data(all_data_r1p3, order)\n",
    "report_results(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab866c",
   "metadata": {},
   "source": [
    "## Case4\n",
    "Data for 1-10 is the average of all Participant annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e0eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSDS\t\tInfo\t\tNon-Red\t\tFlu\t\tOverall\n",
      "PGN-multi\t0.63/0.59*\t0.58/0.56\t0.69/0.70\t0.63/0.62*\n",
      "PGN-both\t0.62/0.64*\t0.61/0.59\t0.68/0.73\t0.64/0.65*\n",
      "BERT-multi\t0.56/0.45\t0.71*/0.61\t0.82/0.80\t0.70*/0.62\n",
      "BERT-both\t0.56/0.48\t0.62*/0.58\t0.78/0.79\t0.66*/0.62\n"
     ]
    }
   ],
   "source": [
    "df4 = (df1 + df2 + df3)/3\n",
    "\n",
    "report_results(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b6ffb",
   "metadata": {},
   "source": [
    "# Create Latex tables for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df79a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number_latex(value: float, p_value: float, other_value: float) -> str:\n",
    "    output_value = f'{value:.2f}'\n",
    "    if float(f'{value:.2f}') >= float(f'{other_value:.2f}'):\n",
    "        if abs(p_value) < 0.05:\n",
    "            output_value += '*'\n",
    "        output_value = r'\\textbf{' + output_value + r'}'\n",
    "    return output_value\n",
    "\n",
    "def get_latex_table(data_df: pd.DataFrame, case: int) -> str:\n",
    "    results = (data_df.mean(axis=0) / 2).to_dict()\n",
    "    p_values = ttest(data_df)\n",
    "    lines = []\n",
    "    lines.append(r'\\multicolumn{5}{c}{Case ' + str(case) + r'} \\\\')\n",
    "    lines.append('\\hline')\n",
    "    options = ('multi', 'both')\n",
    "    for i, architecture in enumerate(('PGN', 'BERT')):\n",
    "        for j, option in enumerate(options):\n",
    "            cells = []\n",
    "            for metric in ('Informativeness', 'Non-redundancy', 'Fluency', 'Overall'):\n",
    "                role_numbers = []\n",
    "                for role in ('user', 'agent'):\n",
    "                    this_value = results[f'{metric}_{architecture}-{option}_{role}']\n",
    "                    other_value = results[f'{metric}_{architecture}-{options[(j+1)%2]}_{role}']\n",
    "                    p_value = p_values[metric][architecture][role].pvalue\n",
    "                    formatted_value = format_number_latex(this_value, p_value, other_value)\n",
    "                    role_numbers.append(formatted_value)\n",
    "                cell = '/'.join(role_numbers)\n",
    "                cells.append(cell)\n",
    "            row = ' & '.join([f'{architecture}-{option}'] + cells) + r' \\\\'\n",
    "            lines.append(row)\n",
    "        lines.append('\\hline')\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00374a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}\n",
      "\\centering\n",
      "\\begin{tabular}{lllll}\n",
      "\\textbf{CSDS} & \\textbf{Info} & \\textbf{Non-Red} & \\textbf{Flu} & \\textbf{Overall} \\\\\n",
      "\\hline\n",
      "\\multicolumn{5}{c}{Case 1} \\\\\n",
      "\\hline\n",
      "PGN-multi & \\textbf{0.63}/0.59 & 0.58/0.55 & \\textbf{0.69}/0.70 & 0.63/0.61 \\\\\n",
      "PGN-both & 0.62/\\textbf{0.64*} & \\textbf{0.61}/\\textbf{0.59} & 0.68/\\textbf{0.74} & \\textbf{0.64}/\\textbf{0.65*} \\\\\n",
      "\\hline\n",
      "BERT-multi & 0.55/0.45 & \\textbf{0.69*}/\\textbf{0.61} & \\textbf{0.82}/\\textbf{0.80} & \\textbf{0.69*}/\\textbf{0.62} \\\\\n",
      "BERT-both & \\textbf{0.56}/\\textbf{0.47} & 0.62/0.58 & 0.78/\\textbf{0.80} & 0.65/\\textbf{0.62} \\\\\n",
      "\\hline\n",
      "\\multicolumn{5}{c}{Case 2} \\\\\n",
      "\\hline\n",
      "PGN-multi & \\textbf{0.62}/0.58 & 0.57/0.56 & \\textbf{0.68}/0.69 & 0.62/0.61 \\\\\n",
      "PGN-both & 0.61/\\textbf{0.62} & \\textbf{0.60}/\\textbf{0.58} & 0.67/\\textbf{0.71} & \\textbf{0.63}/\\textbf{0.64*} \\\\\n",
      "\\hline\n",
      "BERT-multi & \\textbf{0.55}/0.45 & \\textbf{0.70*}/\\textbf{0.60} & \\textbf{0.82}/\\textbf{0.78} & \\textbf{0.69*}/\\textbf{0.61} \\\\\n",
      "BERT-both & \\textbf{0.55}/\\textbf{0.47} & 0.62/0.57 & 0.78/\\textbf{0.78} & 0.65/\\textbf{0.61} \\\\\n",
      "\\hline\n",
      "\\multicolumn{5}{c}{Case 3} \\\\\n",
      "\\hline\n",
      "PGN-multi & \\textbf{0.64}/0.60 & 0.59/0.58 & \\textbf{0.69}/0.72 & \\textbf{0.64}/0.63 \\\\\n",
      "PGN-both & 0.63/\\textbf{0.65*} & \\textbf{0.62}/\\textbf{0.60} & 0.68/\\textbf{0.75} & \\textbf{0.64}/\\textbf{0.67*} \\\\\n",
      "\\hline\n",
      "BERT-multi & \\textbf{0.57}/0.46 & \\textbf{0.72*}/\\textbf{0.62} & \\textbf{0.83}/\\textbf{0.81} & \\textbf{0.71*}/\\textbf{0.63} \\\\\n",
      "BERT-both & \\textbf{0.57}/\\textbf{0.49} & 0.63/0.59 & 0.79/0.80 & 0.67/\\textbf{0.63} \\\\\n",
      "\\hline\n",
      "\\multicolumn{5}{c}{Case 4} \\\\\n",
      "\\hline\n",
      "PGN-multi & \\textbf{0.63}/0.59 & 0.58/0.56 & \\textbf{0.69}/0.70 & 0.63/0.62 \\\\\n",
      "PGN-both & 0.62/\\textbf{0.64*} & \\textbf{0.61}/\\textbf{0.59} & 0.68/\\textbf{0.73} & \\textbf{0.64}/\\textbf{0.65*} \\\\\n",
      "\\hline\n",
      "BERT-multi & \\textbf{0.56}/0.45 & \\textbf{0.71*}/\\textbf{0.61} & \\textbf{0.82}/\\textbf{0.80} & \\textbf{0.70*}/\\textbf{0.62} \\\\\n",
      "BERT-both & \\textbf{0.56}/\\textbf{0.48} & 0.62/0.58 & 0.78/0.79 & 0.66/\\textbf{0.62} \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{\\label{tab:results} Results of the present human evaluation under the four ``cases'' (see Section~\\ref{sec:experimental_setup}).}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "headers = ('CSDS', 'Info', 'Non-Red', 'Flu', 'Overall')\n",
    "preamble = [r'\\begin{table*}', \n",
    "            r'\\centering', \n",
    "            r'\\begin{tabular}{lllll}', \n",
    "            ' & '.join([r'\\textbf{' + el + '}' for el in headers]) + r' \\\\',\n",
    "           '\\hline']\n",
    "body = [get_latex_table(v, k) for k, v in {1: df1, 2: df2, 3: df3, 4: df4}.items()]\n",
    "closing = [r'\\end{tabular}', \n",
    "              r'\\caption{\\label{tab:results} ' + \\\n",
    "              r\"Results of the present human evaluation under the four ``cases'' (see Section~\\ref{sec:experimental_setup}).}\", \n",
    "              r'\\end{table*}']\n",
    "full_table = '\\n'.join(preamble + body + closing)\n",
    "print(full_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b187a8",
   "metadata": {},
   "source": [
    "# Reproducibility scores\n",
    "\n",
    "## Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b7d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_values(data_df: pd.DataFrame) -> list:\n",
    "    results = (data_df.mean(axis=0) / 2).to_dict()\n",
    "    p_values = ttest(data_df)\n",
    "    values = []\n",
    "    for model in ('PGN-multi', 'PGN-both', 'BERT-multi', 'BERT-both'):\n",
    "        for metric in ('Informativeness', 'Non-redundancy', 'Fluency', 'Overall'):\n",
    "            for role in ('user', 'agent'):\n",
    "                values.append(results[f'{metric}_{model}_{role}'])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ebb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_values = [concatenate_values(v) for v in (df1, df2, df3, df4)]\n",
    "case_values = [[case_values[j][i] for j in range(len(case_values))] for i in range(len(case_values[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917ade6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_values = [0.69, 0.65, 0.54, 0.55, 0.70, 0.79, 0.64, 0.66,\n",
    "0.66, 0.69, 0.58, 0.59, 0.73, 0.81, 0.66, 0.70,\n",
    "0.58, 0.56        , 0.66, 0.61 , 0.84, 0.87  , 0.69, 0.68,\n",
    "0.62, 0.60, 0.62, 0.60       , 0.85, 0.87       , 0.70, 0.69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632612a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = r_regression(case_values, orig_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001ebf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_table = [r'\\begin{table*}', r'\\centering', r'\\begin{tabular}{cc}', r'Case & Pearson $\\rho$ \\\\', '\\hline']\n",
    "for i, rho in enumerate(rhos):\n",
    "    pearson_table.append(f'{i+1} & {rho:.2f}' + r' \\\\')\n",
    "pearson_table += [r'\\end{tabular}', \n",
    "              r'\\caption{\\label{tab:pearson} ' + \\\n",
    "              r\"Pearson correlation coefficient between the results of the original experiment and our results.}\", \n",
    "              r'\\end{table*}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c9e0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}\n",
      "\\centering\n",
      "\\begin{tabular}{cc}\n",
      "Case & Pearson $\\rho$ \\\\\n",
      "\\hline\n",
      "1 & 0.90 \\\\\n",
      "2 & 0.89 \\\\\n",
      "3 & 0.90 \\\\\n",
      "4 & 0.90 \\\\\n",
      "\\end{tabular}\n",
      "\\caption{\\label{tab:pearson} Pearson correlation coefficient between the results of the original experiment and our results.}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "pearson_table = '\\n'.join(pearson_table)\n",
    "print(pearson_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb4533",
   "metadata": {},
   "source": [
    "## Fraction of matching both/multi pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f582de",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_orig = {'PGN': orig_values[:16], 'BERT': orig_values[16:]}\n",
    "arch_orig = {k: {'multi': v[:8], 'both': v[8:]} for k, v in arch_orig.items()}\n",
    "arch_orig = {k: {kk: {'Informativeness': vv[0:2], 'Non-redundancy': vv[2:4], 'Fluency': vv[4:6], 'Overall': vv[6:8]} \\\n",
    "                 for kk, vv in v.items()} \\\n",
    "             for k, v in arch_orig.items()}\n",
    "arch_orig = {arch: {option: {metric: {'user': vvv[0], 'agent': vvv[1]} \\\n",
    "                             for metric, vvv in vv.items()} \\\n",
    "                    for option, vv in v.items()} \\\n",
    "             for arch, v in arch_orig.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18d3453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches(v1: float, v2: float, matching_function) -> int:\n",
    "    if matching_function(v1, v2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_matching_list(data_df: pd.DataFrame) -> list:\n",
    "    results = (data_df.mean(axis=0) / 2).to_dict()\n",
    "    options = ('multi', 'both')\n",
    "    scores = []\n",
    "    for i, architecture in enumerate(('PGN', 'BERT')):\n",
    "        cells = []\n",
    "        option_orig = arch_orig[architecture]\n",
    "        for metric in ('Informativeness', 'Non-redundancy', 'Fluency', 'Overall'):\n",
    "            role_numbers = []\n",
    "            for role in ('user', 'agent'):\n",
    "                values = {option: float(str(f\"{results[f'{metric}_{architecture}-{option}_{role}']:.2f}\")) \\\n",
    "                          for option in ('multi', 'both')}\n",
    "                if values['multi'] > values['both']:\n",
    "                    if matches(option_orig['multi'][metric][role], option_orig['both'][metric][role], lambda x,y: x>y) == 1:\n",
    "                        scores.append(1)\n",
    "                    else:\n",
    "                        scores.append(0)\n",
    "                        #print(architecture, metric, role, values['multi'], values['both'])\n",
    "                    #scores.append(int(option_orig['multi'][metric][role] > option_orig['both'][metric][role]))\n",
    "                elif values['multi'] == values['both']:\n",
    "                    if option_orig['multi'][metric][role] == option_orig['both'][metric][role]:\n",
    "                        scores.append(1)\n",
    "                    else:\n",
    "                        scores.append(0)\n",
    "                        #print(architecture, metric, role, values['multi'], values['both'])\n",
    "                    #scores.append(matches(option_orig['multi'][metric][role], option_orig['both'][metric][role], lambda x,y: x==y))\n",
    "                else:\n",
    "                    if option_orig['multi'][metric][role] < option_orig['both'][metric][role]:\n",
    "                        scores.append(1)\n",
    "                    else:\n",
    "                        scores.append(0)\n",
    "                        #print(architecture, metric, role, values['multi'], values['both'])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60d16f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_accuracy(data_df: pd.DataFrame) -> float:\n",
    "    matches = get_matching_list(data_df)\n",
    "    return sum(matches) / len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfaa41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matching_accuracy(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707d7745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matching_accuracy(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bf89aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matching_accuracy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a388cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matching_accuracy(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b85be",
   "metadata": {},
   "source": [
    "## F1 score of statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b418125",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [int(el) for el in \"0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43acbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_statistically_significant(value: float, p_value: float, other_value: float) -> str:\n",
    "    output_value = f'{value:.2f}'\n",
    "    if float(f'{value:.2f}') >= float(f'{other_value:.2f}'):\n",
    "        if abs(p_value) < 0.05:\n",
    "            return 1\n",
    "    return 0\n",
    "        \n",
    "def get_statistical_significance(data_df: pd.DataFrame) -> list:\n",
    "    results = (data_df.mean(axis=0) / 2).to_dict()\n",
    "    p_values = ttest(data_df)\n",
    "    options = ('multi', 'both')\n",
    "    lines = []\n",
    "    for i, architecture in enumerate(('PGN', 'BERT')):\n",
    "        for j, option in enumerate(options):\n",
    "            cells = []\n",
    "            for metric in ('Informativeness', 'Non-redundancy', 'Fluency', 'Overall'):\n",
    "                role_numbers = []\n",
    "                for role in ('user', 'agent'):\n",
    "                    this_value = results[f'{metric}_{architecture}-{option}_{role}']\n",
    "                    other_value = results[f'{metric}_{architecture}-{options[(j+1)%2]}_{role}']\n",
    "                    p_value = p_values[metric][architecture][role].pvalue\n",
    "                    role_numbers.append(is_statistically_significant(this_value, p_value, other_value))\n",
    "                cells += role_numbers\n",
    "            lines += cells\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ce01c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(data_df: pd.DataFrame) -> float:\n",
    "    return f1_score(y_true, get_statistical_significance(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47cb925e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.25', '0.29', '0.25', '0.25']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{get_f1_score(df):.2f}' for df in (df1, df2, df3, df4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a1d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
